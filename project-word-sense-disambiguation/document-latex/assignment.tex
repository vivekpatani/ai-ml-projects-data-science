%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{hyperref} % Used for Hyper Linking
\usepackage{amsmath} % Used for Math
\usepackage{lstautogobble}


% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Python, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	HYPERLINK CUSTOMISATION
%----------------------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#1} % Assignment title
\newcommand{\hmwkDueDate}{Sunday,\ February\ 26,\ 2017} % Due date
\newcommand{\hmwkClass}{CSCI - B\ 659} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Prof. Cavar} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{poosingh/rraavi/vpatani} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	Corpus Selection
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Text Corpus Selection]
We selected our corpuse of text to be \textbf{Driver.}\\

From the below given meanings (\textbf{NLTK}):
\begin{enumerate}
\item The operator of a motor vehicle
\item Someone who drives animals that pull a vehicle
\item A golfer who hits the golf ball with a driver
\item (Computer science) a program that determines how a computer will communicate with a peripheral device
\item A golf club (a wood) with a near vertical face that is used for hitting long shots from the tee
\end{enumerate}
~\\
We are only interested in the \textbf{first}, \textbf{second} and \textbf{fourth} meaning of the words for our disambiguation.
~\\
We have followed 3 approaches to disambiguate the given word:
\begin{itemize}
\item Bag Of Words Approach
\item Naive Bayes
\item Naive Bayes with TF - IDF
\end{itemize}

\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 1 - Bag of Words
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[Approach 1 - Bag of Words]

\begin{subsection}{Model Selection}
Our Bag Of Words approach comes from the family of Lexical Sample Task. This contains a small
set of pre selected words on which we train our model.\\
We first build a vocabulary of words at the begining, find synonyms and antonyms for it, and
put them in a vocabulary collection. This is our feature set. Each vocabulary word (or its
related meaning) represents a position in the feature vector.
\end{subsection}

\begin{subsection}{Optimise Feature Extraction}
While preparing the vocabulary, we use stemming (sometimes stemming adversely affects our output
but we selected to go ahead with it anyway) and lemmatisation.\\
We pass each word of the vocabulary through Stemming and Lemmatisation, followed by looking for similar (synonyms and antonyms) words. This will also contain the word to be disambiguated.
\end{subsection}

\begin{subsection}{Making the Training/Testing Model}
We read each line of the training set and check for similar words from the vocabulary and if it does exist then we mark that location of feature vector as 1, which in the hindisght means that this sentence represents some relation (maybe it is negatively co-related, but it is) and it is good to learn.\\
We prepare the test set in a similar fashion.
\end{subsection}

\begin{subsection}{Results}
Once TimBL trains and test data is given, we need to interpret the result.\\
If on given test data, TimBL classifies the data as 0, then it has correctly classified that data point and 1 means it has incorrectly classified that data point. We had fairly variable results.

As a whole test data set, it has given us an accuracy within in a range of 0\% to 60\%, due to unpredictive nature of IB1 algorithm.\\ On an average we get 5/10 results correctly classified with the correct definition of that word.
\end{subsection}

\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 2 - Naive Bayes
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Approach 2 - Naive Bayes]
\begin{subsection}{Model Selection}
The Naive Bayes assumption comes with a small caveat that the independence assumption discards any relation between features (words) or such entities. We cannot strongly (or at all) represent relation between features and that may sometimes be a big problem but we have gone ahead and used it.
\end{subsection}
\begin{subsection}{Optimisation Feature Extraction and Modeling Data}
For this classification, we generate frequency profiles for each class and use term frequency for evaluating the scores. To generate the frequency profiles, we tokenize the words for which we use NLTK. Consider a scenario where token ends with a punctuation and the same token doesn’t end in punctuation. While counting the frequency for this token, we might want to consider as one token occurring twice, instead of considering them as two different tokens altogether. For this reason, we use regular expressions to pick only the characters in the alphabet and tokenize them and then determine the term frequency. Also, we won’t be calculating the term frequencies for stop words. Once we have the frequency class profiles, we proceed to try and classify the unknown text.\\
The unknown text will be tokenized and for each token of the unknown text, we find out the occurrence of that token in the class and then calculate accordingly. This calculation must be done for all the tokens of the unknown text against each class. Once we have two final scores, we evaluate to find out the higher score and decide which class the unknown text is more likely to fall under.\\~\\

\end{subsection}
\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 3 - Naive Bayes (TF - IDF)
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Approach 3 - Naive Bayes - TF-IDF]
\begin{subsection}{Optimisations and improvements}
One way to optimize our classification is by refining how we calculate the frequency scoring. One other way to do this by using term frequency – inverse document frequency, simply put tf-idf scoring.
\begin{gather*}
					TF-IDF = tf \times idf \\
					idf = \log (N/df) \\
					where, N \rightarrow Total number of documents \\ df \rightarrow document frequency
\end{gather*}
The idea is that, there might be words occurring more frequently and having no significance to the context. To avoid having higher scores for such words, we use tf – idf to limit. It reduces the scoring of such words, depending on the word/term appearing in multiple documents of collection.
Also, in our case, the two text files will be like two Text Collections and each line is like one document.

\end{subsection}
\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	Comparison
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Comparison]
\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[References]
\begin{enumerate}
\item \href{https://web.stanford.edu/~jurafsky/slp3/slides/Chapter18.wsd.pdf}{Stanford Slides}
\item \href{https://github.com/alvations/pywsd}{PYWSD}
\item \href{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}{Wikipedia - Naive Bayes}
\item \href{http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html}{Stanford NLP}
\end{enumerate}
\clearpage
\end{homeworkProblem}

%----------------------------------------------------------------------------------------

\end{document}