%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{hyperref} % Used for Hyper Linking
\usepackage{amsmath} % Used for Math
\usepackage{lstautogobble}
\usepackage{booktabs} % For Tables


% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Python, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	HYPERLINK CUSTOMISATION
%----------------------------------------------------------------------------------------
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#1} % Assignment title
\newcommand{\hmwkDueDate}{Sunday,\ February\ 26,\ 2017} % Due date
\newcommand{\hmwkClass}{CSCI - B\ 659} % Course/class
\newcommand{\hmwkClassTime}{} % Class/lecture time
\newcommand{\hmwkClassInstructor}{Prof. Cavar} % Teacher/lecturer
\newcommand{\hmwkAuthorName}{poosingh/rraavi/vpatani} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	Corpus Selection
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Text Corpus Selection]
We selected our corpuse of text to be \textbf{Driver.}\\

From the below given meanings (\textbf{NLTK}):
\begin{enumerate}
\item The operator of a motor vehicle
\item Someone who drives animals that pull a vehicle
\item A golfer who hits the golf ball with a driver
\item (Computer science) a program that determines how a computer will communicate with a peripheral device
\item A golf club (a wood) with a near vertical face that is used for hitting long shots from the tee
\end{enumerate}
~\\
We are only interested in the \textbf{first}, \textbf{second} and \textbf{fourth} meaning of the words for our disambiguation.
~\\
We have followed 4 approaches to disambiguate the given word:
\begin{itemize}
\item Bag Of Words Approach
\item Collocation Features Approach
\item Naive Bayes
\item Naive Bayes with TF - IDF
\end{itemize}

\textbf{In order to run the code, please refer the Readme.md file.}

\textbf{Also, our code can be found on:} \href{https://github.com/vivekpatani/ai-ml-projects-data-science/tree/master/project-word-sense-disambiguation}{Github}

\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 1 - Bag of Words
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[Approach 1 - Bag of Words]

\begin{subsection}{Model Selection}
Our Bag Of Words approach comes from the family of Lexical Sample Task. This contains a small
set of pre selected words on which we train our model.\\
We first build a vocabulary of words at the begining, find synonyms and antonyms for it, and
put them in a vocabulary collection. This is our feature set. Each vocabulary word (or its
related meaning) represents a position in the feature vector.
\end{subsection}

\begin{subsection}{Optimise Feature Extraction}
While preparing the vocabulary, we use stemming (sometimes stemming adversely affects our output
but we selected to go ahead with it anyway) and lemmatisation.\\
We pass each word of the vocabulary through Stemming and Lemmatisation, followed by looking for similar (synonyms and antonyms) words. This will also contain the word to be disambiguated.
\end{subsection}

\begin{subsection}{Making the Training/Testing Model}
We read each line of the training set and check for similar words from the vocabulary and if it does exist then we mark that location of feature vector as 1, which in the hindisght means that this sentence represents some relation (maybe it is negatively co-related, but it is) and it is good to learn.\\
We prepare the test set in a similar fashion.
\end{subsection}

\begin{subsection}{Results}
TimBL on receiving the input trains and tests the data.\\
We tried different algorithms and the best one it worked was for \textbf{TRIBL2} algorithm with \textbf{Overlap} distance metric.\\
The other closest was leaving the default values for TimBL with a difference of about 5 - 10\% but it was consistent in producing results.\\

Please check comparison for more on results.
\end{subsection}

\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 2 - Collocational Features
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[Approach 2 - Collacational Features]

\begin{subsection}{Model Selection}
The collocational feature model, simply keeps in mind the immediate neighbours and their features.\\
More importantly we record the Part-Of-Speech tags of these neighbours and use it as a feature. We search for similar patterns and train our model. The model contains unique tags that have appeared and the more they appear the stronger is relation.\\
Each feature is represented by the unique tags found in train and testing together. If a tag is not seen before, it is discarded.
\end{subsection}

\begin{subsection}{Optimise Feature Extraction}
While preparing the data, we tried removing the stop words and seems like it is not a good idea. They represent a great deal of data, even though they appear in all sentences, they give you the presence of strong verbs and proper nouns. Frequency Profiles indicated they appear in large quantities. The performance difference was incremental on keeping stop words.
\end{subsection}

\begin{subsection}{Making the Training/Testing Model}
Training is marked by each feature represented as a unique tag (from the Parts-of-Speech Tag) and everytime a sentence of training (or testing) is scanned and contains any of these, it is marked as 1. Last column is given as predicted class.
\end{subsection}

\begin{subsection}{Results}
TimBL is interesting! The IB1 works pretty intuitevly if observed. We gave it dirty test data (i.e. it did not relate or minutely related to either of the trained data to check if we are going right) and obviously it failed horribly but still gave an accuracy of 10\% - 20\%. On the other hand, it trained well, while testing of class we applied (tested) a few of the parameters. We tried \textbf{IB1} with different distance measures and turns out \textbf{Cosine Distance} worked best for us.\\
We tried IGTree where our accuracy by about 35\% with the same distance metric and other distance metric.\\
Please check comparison for more on results.
\end{subsection}

\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 3 - Naive Bayes
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Approach 3 - Naive Bayes]
\begin{subsection}{Model Selection}
The Naive Bayes assumption comes with a small caveat that the independence assumption discards any relation between features (words) or such entities. We cannot strongly (or at all) represent relation between features and that may sometimes be a big problem but we have gone ahead and used it.
\end{subsection}
\begin{subsection}{Optimisation Feature Extraction and Modeling Data}
For this classification, we generate frequency profiles for each class and use term frequency for evaluating the scores. To generate the frequency profiles, we tokenize the words for which we use NLTK. Consider a scenario where token ends with a punctuation and the same token doesn’t end in punctuation. While counting the frequency for this token, we might want to consider as one token occurring twice, instead of considering them as two different tokens altogether. For this reason, we use regular expressions to pick only the characters in the alphabet and tokenize them and then determine the term frequency. Also, we won’t be calculating the term frequencies for stop words. Once we have the frequency class profiles, we proceed to try and classify the unknown text.\\
The unknown text will be tokenized and for each token of the unknown text, we find out the occurrence of that token in the class and then calculate accordingly. This calculation must be done for all the tokens of the unknown text against each class. Once we have two final scores, we evaluate to find out the higher score and decide which class the unknown text is more likely to fall under.\\~\\

\end{subsection}
\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	A - 3 - Naive Bayes (TF - IDF)
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Approach 4 - Naive Bayes - TF-IDF]
\begin{subsection}{Optimisations and improvements}
One way to optimize our classification is by refining how we calculate the frequency scoring. One other way to do this by using term frequency – inverse document frequency, simply put tf-idf scoring.
\begin{gather*}
					TF-IDF = tf \times idf \\
					idf = \log (N/df) \\
					where, N \rightarrow Total number of documents \\ df \rightarrow document frequency
\end{gather*}
The idea is that, there might be words occurring more frequently and having no significance to the context. To avoid having higher scores for such words, we use tf – idf to limit. It reduces the scoring of such words, depending on the word/term appearing in multiple documents of collection.
Also, in our case, the two text files will be like two Text Collections and each line is like one document.

\end{subsection}
\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	Comparison
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{homeworkProblem}[Comparison]
\begin{table}[h!]
  \centering
  \caption{\textbf{Comparison of All Algorithms Used}}
  \label{tab:table1}
  \begin{tabular}{ccc}
    \toprule
    Method & Accuraccy & Parameters\\
    \midrule
    Bag Of Words & 0\% - 60\% & For IB1 - no changes, \\ & & use cosine distance for TRIBL2\\~\\
    Collocational Features & 50\% - 98\% & IB1 - with Cosine Distance Metric\\~\\
    Naive Bayes & Poorly classifies \\ & 4/10 times & Stemming and Lemmatisation\\~\\
    Naive Bayes \\ with TF IDF & Poorly classifies \\ & 2/10 times & Remove Stop Words\\~\\
    \bottomrule
  \end{tabular}
\end{table}

These are the results we obtained. In order to run the code, please refer the Readme.md file.
\end{homeworkProblem}

\clearpage

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}[References]
\begin{enumerate}
\item \href{https://web.stanford.edu/~jurafsky/slp3/slides/Chapter18.wsd.pdf}{Stanford Slides}
\item \href{https://github.com/alvations/pywsd}{PYWSD}
\item \href{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}{Wikipedia - Naive Bayes}
\item \href{http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html}{Stanford NLP}
\item \href{https://github.com/dcavar/python-tutorial-for-ipython/blob/master/notebooks/Python\%20Feature\%20Extraction\%20for\%20Timbl\%20-\%20Class\%20session.ipynb}{Professor Cavars Notebook for Feature Extraction}
\end{enumerate}
\clearpage
\end{homeworkProblem}

%----------------------------------------------------------------------------------------

\end{document}